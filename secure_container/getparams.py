# -*- coding: utf-8 -*-
"""Feature_Extraction_singleurl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rnfH0YOA1VRrs3G6ijpS7n4b75ktswgw
"""

# #Terminal commands
# !pip install tldextract
# !pip install urllib3
# !pip install tld
# !pip install requests
# !pip install python-whois
# !pip install dnspython

"""# Feature Extraction

## Web Scraping with `requests` and `BeautifulSoup`

### What is `BeautifulSoup`?
It is a Python library for pulling data out of HTML and XML files. It provides methods to navigate the document's tree structure and scrape its content.

<!-- ### Our pipeline
<img src='/content/scrape-pipeline.png' width="1024"> -->

# Code
"""

# Imports
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import re
import  tldextract
# from urllib3 import urlparse
import urllib.parse
import urllib3
from urllib.parse import urlparse,quote,unquote,urljoin,urlencode
import tld
from tld import get_tld, is_tld
import string
from collections import Counter
import whois
from datetime import datetime
from xml.etree import ElementTree
import dns.resolver
import csv
# import datetime
from tqdm import tqdm
from time import sleep
import time
from numba import njit, prange
import json
# from datetime import strptime
# from word_with_nlp import nlp_class

input_url = ["www.google.com"]

# Initialise the dict with all None values
def initialize(features):
  features = {
      "url" : None,

      "scheme" : None,
      "hostname" : None,
      "path" : None,
      "netloc" : None,
      "domain" : None,
      "subdomains" : None,
      "tld" : None,

      "length_url" : None,
      "length_hostname" : None,
      'ip' : None,
      'path_extension' : None,

      "nb_dots" : None,
      "nb_hyphens" : None,
      "nb_at" : None,
      "nb_qm" : None,
      "nb_and" : None,
      "nb_or" : None,
      "nb_eq" : None,
      "nb_underscore" : None,
      "nb_tilde" : None,
      "nb_percent" : None,
      "nb_slash" : None,
      "nb_star" : None,
      "nb_colon" : None,
      "nb_comma" : None,
      "nb_semicolumn" : None,
      "nb_dollar" : None,
      "nb_space" : None,
      "nb_www" : None,
      "nb_com" : None,
      "nb_dslash" : None,
      "nb_spl" : None,

      # "nb_redirection" : None,
      # "ratio_intRedirection": None,
      # "ratio_extRedirection": None,
      # "ratio_intErrors": None,
      # "ratio_extErrors": None,
      # "total_requests": None,
      # "int_redirections": None,
      # "ext_redirections": None,
      # "int_errors": None,
      # "ext_errors": None,

      "http_in_path" : None,
      "https_token" : None,
      "ratio_digits_url" : None,
      "ratio_digits_host" : None,
      "punycode" : None,
      "port" : None,
      "tld_in_path" : None,
      "tld_in_subdomain" : None,
      "abnormal_subdomain" : None,
      "nb_subdomains" : None,
      "prefix_suffix" : None,
      "random_domain" : None,
      "shortening_service" : None,

      "length_words_raw" : None,
      "char_repeat" : None,
      "shortest_words_raw" : None,
      "shortest_word_host" : None,
      "shortest_word_path" : None,
      "longest_words_raw" : None,
      "longest_word_host" : None,
      "longest_word_path" : None,
      "avg_words_raw" : None,
      "avg_word_host" : None,
      "avg_word_path" : None,

      "phish_hints" : None,
      "domain_in_brand" : None,
      "brand_in_subdomain" : None,
      "brand_in_path" : None,
      "suspicious_tld" : None,

      "nb_hyperlinks" : None,
      "ratio_intHyperlinks" : None,
      "ratio_extHyperlinks" : None,
      "ratio_nullHyperlinks" : None,
      "ratio_safe_anchors" : None,
      "nb_extCSS" : None,

      "onmouseover": None,
      "right_click_disabled": None,
      "empty_title": None,
      "domain_in_title": None,
      "domain_with_copyright": None,

      "whois_registered_domain": None,
      "domain_registration_length": None,
      "domain_age": None,

      "web_traffic": None,
      "dns_record": None,
      "google_index": None,
      # "page_rank": None,

      "sfh" : None,
      "iframe" : None,
      "popup_window" : None,

      "login_form": None,
      "external_favicon": None,
      "links_in_tags": None,
      "submit_email": None,
      "ratio_intMedia": None,
      "ratio_extMedia": None
    }
  return features

"""## Feature Extractions
### Address bar related features
"""

# Function to extract different components of URL
def extract_url_components(features_overall, listOrDict=False):
  features = {
      "scheme" : None,
      "hostname" : None,
      "path" : None,
      "netloc" : None,
      "domain" : None,
      "subdomains" : None,
      "tld" : None
  }
  # features = features_overall
  try:
    # Method 1
    url = features_overall["url"]
    parsed_url = urllib.parse.urlparse(url)
    features["scheme"] = parsed_url.scheme
    features["hostname"] = parsed_url.hostname

    # print(f"{tldextract.extract(url).registered_domain}   {features['hostname']}")
    # eg: crestonwood.com   www.crestonwood.com

    features["path"] = parsed_url.path
    features["netloc"] = parsed_url.netloc
    # domain = parsed_url.netloc.split('.')[-2]  # example
    # subdomain = parsed_url.netloc.split('.')[0] if len(parsed_url.netloc.split('.')) > 2 else ""  # www
    # print(f"Method 1\n{domain}  ,  {subdomain}  ,  {scheme}  ,  {hostname}  ,  {path}  ,  {netloc}\n")

    # # Method 2
    # subdomains = hostname.split('.')[:-2]
    # domain_parts = hostname.split('.')
    # domain = domain_parts[-2] if len(domain_parts) > 1 else domain_parts[0]
    # print(f"Method 2\n{domain}  ,  {subdomains}\n")

    # Method 3
    extracted_info = tldextract.extract(url)
    features["domain"] = extracted_info.domain
    subdomain = extracted_info.subdomain
    features["subdomains"] = subdomain.split('.')
    features["tld"] = extracted_info.suffix
    # print(f"M 3\n{domain}  ,  {subdomain}   ,   {tld}\n")

    # Method 4
    # res = get_tld(url, as_object=True) #Get the root as an object
    # fld = res.fld
    # domain = res.domain
    # subdomain = res.subdomain
    # tld = res.tld
    # print(f"M 4\n{domain}  ,  {subdomain}   ,   {tld}\n")
    # print(is_tld(res.tld), "\n")

    # Method 3 > Method 4 >> Method 2 ~ Method 1

    # print(f"url : {features['url']}")
    # print(f"scheme : {features['scheme']}")
    # print(f"hostname : {features['hostname']}")
    # print(f"path : {features['path']}")
    # print(f"netloc : {features['netloc']}")
    # print(f"domain : {features['domain']}")
    # print(f"subdomains : {features['subdomains']}")
    # print(f"tld : {features['tld']}")

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

#function for url parameters(length_url,length_hostname etc)
def get_url_features(features_overall, listOrDict=False):
  features = {
      "length_url" : None,
      "length_hostname" : None,
      'ip' : None,
      'path_extension' : None,

  }
  # features = features_overall
  try:
    url = features_overall["url"]
    features["length_url"] = len(url)
    # features["length_hostname"] = len(tldextract.extract(url).registered_domain)
    features["length_hostname"] = len(features_overall["hostname"])

    # ip_pattern = re.compile(r'(\d{1,3}\.){3}\d{1,3}')
    ip_pattern = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)|'  # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|'
        '[0-9a-fA-F]{7}', url)  # Ipv6
    # features['ip'] = bool(ip_pattern.search(url))
    if ip_pattern:
      features['ip'] = 1
    else:
      features['ip'] = 0

    path = features_overall["path"]
    # path = urlparse(url).path
    if '.' in path:
        extension = path.split('.')[-1]
    else:
        extension = None
    if extension:
      features['path_extension'] = 1
    else:
      features['path_extension'] = 0

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

#function for character counts in URL
def url_char_count(features_overall, listOrDict=False):#default return datatype is dict
  features = {
      "nb_dots" : None,
      "nb_hyphens" : None,
      "nb_at" : None,
      "nb_qm" : None,
      "nb_and" : None,
      "nb_or" : None,
      "nb_eq" : None,
      "nb_underscore" : None,
      "nb_tilde" : None,
      "nb_percent" : None,
      "nb_slash" : None,
      "nb_star" : None,
      "nb_colon" : None,
      "nb_comma" : None,
      "nb_semicolumn" : None,
      "nb_dollar" : None,
      "nb_space" : None,
      "nb_www" : None,
      "nb_com" : None,
      "nb_dslash" : None,
      "nb_spl" : None
  }
  # features = features_overall
  try:
    url = features_overall["url"]
    features["nb_dots"] = url.count('.')
    features["nb_hyphens"] = url.count('-')
    features["nb_at"] = url.count('@')
    features["nb_qm"] = url.count('?')
    features["nb_and"] = url.count('&')
    features["nb_or"] = url.count('|')
    features["nb_eq"] = url.count('=')
    features["nb_underscore"] = url.count('_')
    features["nb_tilde"] = url.count('~')
    features["nb_percent"] = url.count('%')
    single_slash = re.compile(r'[^/]/[^/]')
    features["nb_slash"] = len(re.findall(single_slash, url))
    features["nb_star"] = url.count('*')
    features["nb_colon"] = url.count(':')
    features["nb_comma"] = url.count(',')
    features["nb_semicolumn"] = url.count(';')
    features["nb_dollar"] = url.count('$')
    features["nb_space"] = url.count(' ') + url.count('%20')
    features["nb_www"] = url.count('www')
    # w_domain + w_path + w_subdomain
    features["nb_com"] = f"{features_overall['domain'] + features_overall['path']}".count('.com')
    features["nb_com"] += 1 if 'com' in features_overall['subdomains'] else 0
    # features["nb_com"] = url.count('.com')
    # features["nb_dslash"] = url.count('//')
    dslash_list=[x.start(0) for x in re.finditer('//', url)]
    if dslash_list[len(dslash_list)-1]>6:
        features["nb_dslash"] = 1
    else:
        features["nb_dslash"] = 0
    features["nb_spl"] = sum(c in "!@#$%^&*()_+=-`~[]{}|;:'\",.<>?/" for c in url)
  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

def get_redirection_and_error_features(features_overall, listOrDict=False):#default return datatype is dict
    features = {
        "nb_redirection" : None,
        "ratio_intRedirection": 0.0,
        "ratio_extRedirection": 0.0,
        "ratio_intErrors": 0.0,
        "ratio_extErrors": 0.0,
        "total_requests": 0,
        "int_redirections": 0,
        "ext_redirections": 0,
        "int_errors": 0,
        "ext_errors": 0
    }

    def is_internal(url, base_hostname):
        return urlparse(url).hostname == base_hostname

    def analyze_url(url, base_hostname):
        try:
            response = requests.get(url, allow_redirects=True)
            features["total_requests"] += 1
            features["nb_redirection"] = len(response.history)

            if response.history:
                for resp in response.history:
                    if 300 <= resp.status_code < 400:
                        if is_internal(resp.url, base_hostname):
                            features["int_redirections"] += 1
                        else:
                            features["ext_redirections"] += 1

            final_url = response.url
            if 400 <= response.status_code < 600:
                if is_internal(final_url, base_hostname):
                    features["int_errors"] += 1
                else:
                    features["ext_errors"] += 1

        except requests.RequestException as e:
            print(f"Request failed: {e}")
            features["total_requests"] += 1  # Count the failed request
            if is_internal(url, base_hostname):
                features["int_errors"] += 1
            else:
                features["ext_errors"] += 1

    try:
        url = features_overall["url"]
        path = features_overall["path"]
        base_hostname = features_overall["hostname"]

        # Analyze the initial URL
        analyze_url(url, base_hostname)

        # Compute ratios
        ImpFeatures={}
        total_requests = features["total_requests"] if features["total_requests"] > 0 else 1  # Avoid division by zero

        ImpFeatures["url"] = url
        ImpFeatures["ratio_intRedirection"] = features["int_redirections"] / total_requests
        ImpFeatures["ratio_extRedirection"] = features["ext_redirections"] / total_requests
        ImpFeatures["ratio_intErrors"] = features["int_errors"] / total_requests
        ImpFeatures["ratio_extErrors"] = features["ext_errors"] / total_requests


    except requests.RequestException as e:
        print(e)
    if listOrDict:
        return features.values()
    else:
        features_overall.update(features) # Directly updates overall dictionary with current features
        return features #Returns only current features

# Function to extract path and hostname features
def path_hostname_features(features_overall, listOrDict=False):
  features = {
      "http_in_path" : None,
      "https_token" : None,
      "ratio_digits_url" : None,
      "ratio_digits_host" : None,
      "punycode" : None,
      "port" : None,
      "tld_in_path" : None,
      "tld_in_subdomain" : None,
      "abnormal_subdomain" : None,
      "nb_subdomains" : None,
      "prefix_suffix" : None,
      "random_domain" : None,
      "shortening_service" : None
  }
  try:
    url = features_overall["url"]
    path = features_overall["path"]
    scheme = features_overall["scheme"]
    hostname = features_overall["hostname"]
    domain = features_overall["domain"]
    subdomains = features_overall["subdomains"]

    features["http_in_path"] = 'http' in path
    features["https_token"] = 'https' in scheme

    # Calculate the ratio of digits to total characters in the URL
    total_chars_url = len(url)
    digit_count_url = sum(c.isdigit() for c in url)
    features["ratio_digits_url"] = digit_count_url / total_chars_url if total_chars_url > 0 else 0

    # Calculate the ratio of digits to total characters in the hostname
    total_chars_host = len(hostname)
    digit_count_host = sum(c.isdigit() for c in hostname)
    features["ratio_digits_host"] = digit_count_host / total_chars_host if total_chars_host > 0 else 0

    # Check if the URL uses Punycode encoding
    features["punycode"] = 'xn--' in hostname

    # Check if the URL uses a specific port number
    features["port"] = urllib.parse.urlparse(url).port is not None


    tlds = ['.com', '.org', '.net', '.info', '.biz', '.gov', '.edu', '.co', '.us']
    tlds.append(f"{features_overall['tld']}")
    features["tld_in_path"] = any(tld in path for tld in tlds)

    # Check if TLD is in the subdomain
    features["tld_in_subdomain"] = any(tld in subdomain for subdomain in subdomains for tld in tlds)

    # Check for abnormal subdomain structure - starting with wwww-, wwNN
    features["abnormal_subdomain"] = bool(re.search('(http[s]?://(w[w]?|\d))([w]?(\d|-))',url))

    # Count the number of subdomains
    features["nb_subdomains"] = len(subdomains)

    # Check if the hostname contains a hyphen
    features["prefix_suffix"] = '-' in hostname

    features["random_domain"] = bool(re.match(r'^[a-z0-9]{10,}$', domain))

    # Check if the URL uses a URL shortening service
    shortening_services = [
        'bit.ly', 'goo.gl', 't.co', 'tinyurl.com', 'is.gd', 'cli.gs',
        'yfrog.com', 'migre.me', 'ff.im', 'tiny.cc', 'url4.eu', 'twit.ac',
        'su.pr', 'twurl.nl', 'snipurl.com', 'short.to', 'budurl.com',
        'ping.fm', 'post.ly', 'just.as', 'bkite.com', 'snipr.com', 'fic.kr',
        'loopt.us', 'doiop.com', 'short.ie', 'b33.fr', 'u.nu', 'sp2.ro',
        'tr.im', 'u.im', 'j.mp', 'bit.do', 'lnkd.in', 'db.tt', 'qr.ae',
        'adf.ly', 'bitly.com', 'cur.lv', 'ity.im', 'q.gs', 'po.st', 'bc.vc',
        'twitthis.com', 'u.to', 'j2j.de', 'dlvr.it', 'oo.gl', 'v.gd', 'link.zip.net'
    ]
    features["shortening_service"] = any(service in hostname for service in shortening_services)

    # # Method 2 for finding shortening service. but this is for all redirections not just shortening service
    # response = urllib3.urlopen(url)
    # # final_url != original_url if redirected
    # final_url = response.geturl()
    # # response_code will be 302 for redirects
    # response_code = response.getcode()

    # if response_code == 302:
    #     # redirected so this may a short url
    #     pass
    # else:
    #     # this is not a short url
    #     pass

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Fuction for word features
def word_features(features_overall, listOrDict=False):
  features = {
      "length_words_raw" : None,
      "char_repeat" : None,
      "shortest_words_raw" : None,
      "shortest_word_host" : None,
      "shortest_word_path" : None,
      "longest_words_raw" : None,
      "longest_word_host" : None,
      "longest_word_path" : None,
      "avg_words_raw" : None,
      "avg_word_host" : None,
      "avg_word_path" : None
  }

  def get_word_stats(text):
        words = re.findall(r'\w+', unquote(text))
        if not words:
            return 0, 0, 0, 0
        # char_repeat=len(set(words))!=len(words)
        total_length = sum(len(word) for word in words)
        shortest_word_length = min(len(word) for word in words)
        longest_word_length = max(len(word) for word in words)
        avg_word_length = total_length / len(words)
        return total_length, shortest_word_length, longest_word_length, avg_word_length
  def __all_same(items):
        return all(x == items[0] for x in items)

  try:
    url = features_overall["url"]
    path = features_overall["path"] or ''
    hostname = features_overall["hostname"] or ''

    total_length, shortest_word_length, longest_word_length, avg_word_length = get_word_stats(url)
    _, shortest_word_host, longest_word_host, avg_word_host = get_word_stats(hostname)
    _, shortest_word_path, longest_word_path, avg_word_path = get_word_stats(path)

    features["length_words_raw"]=total_length

    repeat = {'2': 0, '3': 0, '4': 0, '5': 0}
    part = [2, 3, 4, 5]

    for word in url:
        for char_repeat_count in part:
            for i in range(len(word) - char_repeat_count + 1):
                sub_word = word[i:i + char_repeat_count]
                if __all_same(sub_word):
                    repeat[str(char_repeat_count)] = repeat[str(char_repeat_count)] + 1
    features["char_repeat"]=sum(list(repeat.values()))
    features["shortest_words_raw"]=shortest_word_length
    features["shortest_word_host"]=shortest_word_host
    features["shortest_word_path"]=shortest_word_path
    features["longest_words_raw"]=longest_word_length
    features["longest_word_host"]=longest_word_host
    features["longest_word_path"]=longest_word_path
    features["avg_words_raw"]=avg_word_length
    features["avg_word_host"]=avg_word_host
    features["avg_word_path"]=avg_word_path



  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Function to extract content and metadata features
def phish_identity_features(features_overall, listOrDict=False):
  features = {
      "phish_hints" : None,
      "domain_in_brand" : None,
      "brand_in_subdomain" : None,
      "brand_in_path" : None,
      "suspicious_tld" : None,
  }
  try:
    url = features_overall["url"]
    path = features_overall["path"]
    scheme = features_overall["scheme"]
    hostname = features_overall["hostname"]
    domain = features_overall["domain"]
    subdomains = features_overall["subdomains"]
    tld = features_overall["tld"]

    # Check for phishing hints or keywords
    phishing_hints = ['login', 'verify', 'secure', 'account', 'update', 'banking', 'signin', 'password',
                      'urgent', 'verification required', 'invoice', 'important', 'action required']
    phishing_hints.extend(['wp', 'includes', 'admin', 'content', 'site', 'images', 'js',
                           'alibaba', 'css', 'myaccount', 'dropbox', 'themes', 'plugins', 'view'])
    count = 0
    for hint in phishing_hints:
        count += url.lower().count(hint)
    features["phish_hints"] = count

    # Check if the domain name contains a brand name
    brands = ['accenture', 'activisionblizzard', 'adidas', 'adobe', 'adultfriendfinder', 'agriculturalbankofchina',
              'akamai', 'alibaba', 'aliexpress', 'alipay', 'alliance', 'alliancedata', 'allianceone', 'allianz', 'alphabet',
              'amazon', 'americanairlines', 'americanexpress', 'americantower', 'andersons', 'apache', 'apple', 'arrow',
              'ashleymadison', 'audi', 'autodesk', 'avaya', 'avisbudget', 'avon', 'axa', 'badoo', 'baidu', 'bank', 'bankofamerica',
              'bankofchina', 'bankofnewyorkmellon', 'barclays', 'barnes', 'bbc', 'bbt', 'bbva', 'bebo', 'benchmark', 'bestbuy',
              'bim', 'bing', 'biogen', 'blackstone', 'blogger', 'blogspot', 'bmw', 'bnpparibas', 'boeing', 'booking', 'broadcom',
              'burberry', 'caesars', 'canon', 'cardinalhealth', 'carmax', 'carters', 'caterpillar', 'cheesecakefactory',
              'chinaconstructionbank', 'cinemark', 'cintas', 'cisco', 'citi', 'citigroup', 'cnet', 'coca-cola', 'colgate',
              'colgate-palmolive', 'columbiasportswear', 'commonwealth', 'communityhealth', 'continental', 'dell', 'deltaairlines',
              'deutschebank', 'disney', 'dolby', 'dominos', 'donaldson', 'dreamworks', 'dropbox', 'eastman', 'eastmankodak', 'ebay',
              'edison', 'electronicarts', 'equifax', 'equinix', 'expedia', 'express', 'facebook', 'fedex', 'flickr', 'footlocker',
              'ford', 'fordmotor', 'fossil', 'fosterwheeler', 'foxconn', 'fujitsu', 'gap', 'gartner', 'genesis', 'genuine',
              'genworth', 'gigamedia', 'gillette', 'github', 'global', 'globalpayments', 'goodyeartire', 'google', 'gucci',
              'harley-davidson', 'harris', 'hewlettpackard', 'hilton', 'hiltonworldwide', 'hmstatil', 'honda', 'hsbc', 'huawei',
              'huntingtonbancshares', 'hyundai', 'ibm', 'ikea', 'imdb', 'imgur', 'ingbank', 'insight', 'instagram', 'intel',
              'jackdaniels', 'jnj', 'jpmorgan', 'jpmorganchase', 'kelly', 'kfc', 'kindermorgan', 'lbrands', 'lego', 'lennox',
              'lenovo', 'lindsay', 'linkedin', 'livejasmin', 'loreal', 'louisvuitton', 'mastercard', 'mcdonalds', 'mckesson',
              'mckinsey', 'mercedes-benz', 'microsoft', 'microsoftonline', 'mini', 'mitsubishi', 'morganstanley', 'motorola',
              'mrcglobal', 'mtv', 'myspace', 'nescafe', 'nestle', 'netflix', 'nike', 'nintendo', 'nissan', 'nissanmotor', 'nvidia',
              'nytimes', 'oracle', 'panasonic', 'paypal', 'pepsi', 'pepsico', 'philips', 'pinterest', 'pocket', 'pornhub', 'porsche',
              'prada', 'rabobank', 'reddit', 'regal', 'royalbankofcanada', 'samsung', 'scotiabank', 'shell', 'siemens', 'skype',
              'snapchat', 'sony', 'soundcloud', 'spiritairlines', 'spotify', 'sprite', 'stackexchange', 'stackoverflow', 'starbucks',
              'swatch', 'swift', 'symantec', 'synaptics', 'target', 'telegram', 'tesla', 'teslamotors', 'theguardian', 'homedepot',
              'piratebay', 'tiffany', 'tinder', 'tmall', 'toyota', 'tripadvisor', 'tumblr', 'twitch', 'twitter', 'underarmour',
              'unilever', 'universal', 'ups', 'verizon', 'viber', 'visa', 'volkswagen', 'volvocars', 'walmart', 'wechat', 'weibo',
              'whatsapp', 'wikipedia', 'wordpress', 'yahoo', 'yamaha', 'yandex', 'youtube', 'zara', 'zebra', 'iphone', 'icloud',
              'itunes', 'sinara', 'normshield', 'bga', 'sinaralabs', 'roksit', 'cybrml', 'turkcell', 'n11', 'hepsiburada', 'migros']
    features["domain_in_brand"] = any(brand in domain for brand in brands)

    # Check if a brand name is found in the subdomain
    features["brand_in_subdomain"] = any(('.' + brand + '.') in subdomain for subdomain in subdomains for brand in brands)

    # Check if a brand name is found in the path
    features["brand_in_path"] = any(('.' + brand + '.') in path for brand in brands)

    # Check if the TLD is commonly associated with phishing
    suspecious_tlds = ['cn', 'ru', 'cf', 'gq',
        'fit','tk', 'gp', 'ga', 'work', 'ml', 'date', 'wang', 'men', 'icu', 'online', 'click', # Spamhaus
        'country', 'stream', 'download', 'xin', 'racing', 'jetzt',
        'ren', 'mom', 'party', 'review', 'trade', 'accountants',
        'science', 'work', 'ninja', 'xyz', 'faith', 'zip', 'cricket', 'win',
        'accountant', 'realtor', 'top', 'christmas', 'gdn', # Shady Top-Level Domains
        'link', # Blue Coat Systems
        'asia', 'club', 'la', 'ae', 'exposed', 'pe', 'go.id', 'rs', 'k12.pa.us', 'or.kr',
        'ce.ke', 'audio', 'gob.pe', 'gov.az', 'website', 'bj', 'mx', 'media', 'sa.gov.au' # statistics
        ]
    # suspicious_tld = any(hostname.endswith(tld) for tld in suspicious_tlds)
    features["suspicious_tld"] = not(is_tld(tld))

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Function to extract Hyperlink and CSS Features
def hyperlink_css_feaures(features_overall, soup, listOrDict=False):
  features = {
      "nb_hyperlinks" : None,
      "ratio_intHyperlinks" : None,
      "ratio_extHyperlinks" : None,
      "ratio_nullHyperlinks" : None,
      "ratio_safe_anchors" : None,
      "nb_extCSS" : None
  }
  try:
    url = features_overall["url"]

    # Find all hyperlinks in the webpage
    links = soup.find_all('a')
    features["nb_hyperlinks"] = len(links)

    # Find all internal, external, and null hyperlinks
    internal_links = []
    external_links = []
    null_links = []
    safe_anchors = []

    base_url = f'://{features_overall["netloc"]}'
    # Null_format = ["", "#", "#nothing", "#doesnotexist", "#null", "#void", "#whatever",
    #            "#content", "javascript::void(0)", "javascript::void(0);", "javascript::;", "javascript"]

    for link in links:
        href = link.get('href')
        if not href or href.strip() == '':
            null_links.append(href)
        else:
            if href.startswith('#'):
                safe_anchors.append(href)
            if href.startswith('/') or base_url in href or href.startswith('#'):
                internal_links.append(href)
            else:
                external_links.append(href)
    # print(internal_links)
    # print(external_links)
    # Calculate ratios
    features["ratio_intHyperlinks"] = len(internal_links) / features["nb_hyperlinks"] if features["nb_hyperlinks"] > 0 else 0
    features["ratio_extHyperlinks"] = len(external_links) / features["nb_hyperlinks"] if features["nb_hyperlinks"] > 0 else 0
    features["ratio_nullHyperlinks"] = len(null_links) / features["nb_hyperlinks"] if features["nb_hyperlinks"] > 0 else 0
    features["ratio_safe_anchors"] = len(safe_anchors) / features["nb_hyperlinks"] if features["nb_hyperlinks"] > 0 else 0


    # Find all external CSS files linked in the webpage
    # css_links = soup.find_all('link', rel='stylesheet')
    # features["nb_extCSS"] = len([link for link in css_links if link.get('href') and not link.get('href').startswith('/')])
    ext_css_links = []
    for link in soup.find_all('link', rel='stylesheet'):
        dots = [x.start(0) for x in re.finditer('\.', link['href'])]
        if not (features_overall["hostname"] in link['href'] or features_overall["domain"] in link['href'] or len(dots) == 1 or not link['href'].startswith('http')):
            ext_css_links.append(link['href'])

    features["nb_extCSS"] = len(ext_css_links)

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Function to extract Hyperlink and CSS Features
def interaction_features(features_overall, response, listOrDict=False):
  features = {
        "onmouseover": None,
        "right_click_disabled": None,
        "empty_title": None,
        "domain_in_title": None,
        "domain_with_copyright": None
  }
  try:
    url = features_overall["url"]

    '''
    response = requests.get(url)
    response.raise_for_status()  # Raise an error for bad status codes
    '''
    soup = BeautifulSoup(response.text, 'html.parser')

    # Check for 'onmouseover' event
    # if soup.find_all(onmouseover=True):
    #     features["onmouseover"] = True
    # else:
    #     features["onmouseover"] = False
    content = soup.get_text()
    features["onmouseover"] = ('onmouseover="window.status=' in str(content).lower().replace(" ",""))


    # Check for right-click disabled
    if "contextmenu" in response.text or soup.find_all(oncontextmenu=True):
        features["right_click_disabled"] = True
    else:
        features["right_click_disabled"] = False

    # Check for empty title
    # title_tag = soup.find('title')
    # if not title_tag or not title_tag.string.strip():
    #     features["empty_title"] = True
    # else:
    #     features["empty_title"] = False
    title = ""
    try:
        title = soup.title.string
        if not title or not title.strip():
            features["empty_title"] = True
        else:
            features["empty_title"] = False
    except:
        features["empty_title"] = True

    # Check if domain name is in the title
    domain = features_overall['domain']
    # if domain and title_tag and domain in title_tag.string:
    #     features["domain_in_title"] = True
    # else:
    #     features["domain_in_title"] = False
    if domain and title and domain in title:
        features["domain_in_title"] = True
    else:
        features["domain_in_title"] = False

    # Check if domain name is associated with a copyright
    # if domain and soup.find_all(text=lambda text: text and domain in text and '©' in text):
    #     features["domain_with_copyright"] = True
    # else:
    #     features["domain_with_copyright"] = False
    try:
        content = str(content)
        m = re.search(u'(\N{COPYRIGHT SIGN}|\N{TRADE MARK SIGN}|\N{REGISTERED SIGN})', content)
        _copyright = content[m.span()[0]-50:m.span()[0]+50]
        if domain.lower() in _copyright.lower():
            features["domain_with_copyright"] = False
        else:
            features["domain_with_copyright"] = True
    except:
        features["domain_with_copyright"] = False


  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Function to extract Hyperlink and CSS Features
import json

def whois_features(features_overall, listOrDict=False):
  features = {
      "whois_registered_domain": None,
      "domain_registration_length": None,
      "domain_age": None
  }
  try:
    url = features_overall["url"]
    # domain = whois.whois(urlparse(url).hostname)
    # Check if the domain is registered according to WHOIS
    # features["whois_registered_domain"] = bool(domain.domain_name)

    domain_with_tld = features_overall["domain"] + "." + features_overall["tld"]
    domain = whois.whois(domain_with_tld)
    # print(domain)
    features["whois_registered_domain"] = True
    if type(domain.domain_name) == list:
            for host in domain.domain_name:
                if re.search(host.lower(), domain_with_tld):
                    features["whois_registered_domain"] = False
                    break
    else:
        if re.search(domain.domain_name.lower(), domain_with_tld):
              features["whois_registered_domain"] = False
              # break

    # Calculate domain registration length
    if domain.creation_date:
        # print(domain.creation_date,domain.expiration_date)
        if type(domain.creation_date)==list:
          creation_date=domain.creation_date[0]
        else:
          creation_date=domain.creation_date

    if domain.expiration_date:
        try:
          if type(domain.expiration_date)==list:
            # expiration_date=domain.expiration_date[0]
            expiration_date = min(domain.expiration_date)
          else:
            expiration_date = domain.expiration_date

          # registration_length = (expiration_date - creation_date).days

          # today = datetime.strptime(time.strftime('%Y-%m-%d'), '%Y-%m-%d')
          # registration_length = abs((expiration_date - today).days)

          std_date = datetime.strptime("2024-08-01", '%Y-%m-%d')
          registration_length = abs((expiration_date - std_date).days)

          features["domain_registration_length"] = registration_length
        except Exception as e:
          print(e)
          features["domain_registration_length"] = -1
    else:
        features["domain_registration_length"] = 0

    # Calculate domain age
    if domain.creation_date:
        today = datetime.now()
        domain_age = (today - creation_date).days
        features["domain_age"] = domain_age

    # # New logic. But the API is down. So retaining the old one
    # url = domain_with_tld.split("//")[-1].split("/")[0].split('?')[0]
    # show = "https://input.payapi.io/v1/api/fraud/domain/age/" + url
    # r = requests.get(show)

    # if r.status_code == 200:
    #     data = r.text
    #     json_data = json.loads(data)
    #     result = json_data['result']
    #     if result == None:
    #         features["domain_age"] = -2
    #     else:
    #         features["domain_age"] = result
    # else:
    #     features["domain_age"] = -1

  except requests.RequestException as e:
    print(e)
  except:
    pass
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

# Function to extract Hyperlink and CSS Features
def get_web_traffic_and_index_features(features_overall, listOrDict=False):
    features = {
      "web_traffic": None,
      "dns_record": None,
      "google_index": None,
      # "page_rank": None
    }

    url = features_overall["url"]
    domain = features_overall["hostname"]

    try:
        rank = BeautifulSoup(urllib.request.urlopen("http://data.alexa.com/data?cli=10&dat=s&url=" + url).read(), "xml").find("REACH")['RANK']
        features["web_traffic"] = int(rank)
    except:
        features["web_traffic"] = 0


    # # DNS HISTORY
    # try:
    #     if dns.resolver.resolve(domain, 'A'):
    #         features["dns_record"] = True
    #     else:
    #         features["dns_record"] = False
    # except dns.resolver.NXDOMAIN:
    #     pass
    # except dns.resolver.NoAnswer:
    #     pass

    domain_with_tld = features_overall["domain"] + "." + features_overall["tld"]
    try:
        nameservers = dns.resolver.query(domain_with_tld, 'NS')
        if len(nameservers) > 0:
            features["dns_record"] = False
        else:
            features["dns_record"] = True
    except:
        features["dns_record"] = True


    user_agent =  'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'
    headers = {'User-Agent' : user_agent}
    query = {'q': 'site:' + url}
    google = "https://www.google.com/search?" + urlencode(query)
    data = requests.get(google, headers=headers)
    data.encoding = 'ISO-8859-1'
    soup = BeautifulSoup(str(data.content), "html.parser")
    try:
        if 'Our systems have detected unusual traffic from your computer network.' in str(soup):
            features["google_index"] = -1
        else:
            check = soup.find(id="rso").find("div").find("div").find("a")
            # print(check)
            if check and check['href']:
                features["google_index"] = 0
            else:
                features["google_index"] = 1
    except AttributeError:
        features["google_index"] = 1

    # Page Rank from OPR
    # key = 'Add your OPR API key here'
    # page_rank_url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain_with_tld
    # try:
    #     request = requests.get(page_rank_url, headers={'API-OPR': key})
    #     result = request.json()
    #     result = result['response'][0]['page_rank_integer']
    #     if result:
    #         features["page_rank"] = result
    #     else:
    #         features["page_rank"] = 0
    # except:
    #     features["page_rank"] = -1

    if listOrDict:
      return features.values()
    else:
      features_overall.update(features) # Directly updates overall dictionary with current features
      return features #Returns only current features

# Function to extract Script and Frame Features
def script_frame_features(features_overall, soup, listOrDict=False):
  features = {
      "sfh" : None,
      "iframe" : None,
      "popup_window" : None
  }
  try:
    url = features_overall["url"]
    baseurl = f'://{features_overall["netloc"]}'
    # Check for suspicious form handlers
    forms = soup.find_all('form')
    sfh = 0 # Legitimate
    # Null_format = ["", "#", "#nothing", "#doesnotexist", "#null", "#void", "#whatever",
    #         "#content", "javascript::void(0)", "javascript::void(0);", "javascript::;", "javascript"]
    for form in forms:
      action = form.get('action')
      # # sfh = any(form.get('action') in [None, "", "about:blank"] or form.get('action', '').startswith('http') and not form.get('action', '').startswith(base_url) for form in forms)
      # if action in [None, "", "about:blank"]:
      if action in [None, "", "about:blank", "javascript::void(0)", "javascript::void(0);", "javascript::;", "javascript"]:
        sfh = 1 # Phishing
        break
      elif not (baseurl in action or action.startswith('/')):
        sfh = 0.5 # Suspicious
    features["sfh"] = sfh

    # Check for iframes
    # iframes = soup.find_all('iframe')
    # features["iframe"] = len(iframes) > 0

    features["iframe"] = False
    for iframe in soup.find_all('iframe', width=True, height=True, frameborder=True):
      if iframe['width'] == "0" and iframe['height'] == "0" and iframe['frameborder'] == "0":
        features["iframe"] = True
        break
    if not features["iframe"]:
      for iframe in soup.find_all('iframe', width=True, height=True, border=True):
        if iframe['width'] == "0" and iframe['height'] == "0" and iframe['border'] == "0":
          features["iframe"] = True
          break
    if not features["iframe"]:
      for iframe in soup.find_all('iframe', width=True, height=True, style=True):
        if iframe['width'] == "0" and iframe['height'] == "0" and iframe['style'] == "0":
          features["iframe"] = True
          break

    # Check for popup windows (typically indicated by JavaScript code)
    scripts = soup.find_all('script')
    features["popup_window"] = False
    features["popup_window"] = any('window.open' in script.text for script in scripts)
    if not features["popup_window"]:
      content = soup.get_text()
      if "prompt(" in str(content).lower():
        features["popup_window"] = True

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

def get_page_features(features_overall, soup, listOrDict=False):
  features = {
      "login_form": None,
      "external_favicon": False,
      "links_in_tags": None,
      "submit_email": False,
      "ratio_intMedia": None,
      "ratio_extMedia": None
  }
  try:
    url = features_overall["url"]
    # Fetch the webpage content
    # html_content = response.text
    # soup = BeautifulSoup(html_content, 'html.parser')

    # Check for login form
    # forms = soup.find_all('form')
    forms = soup.find_all('form', action=True)
    for form in forms:
        input_types = [input_.get('type', '').lower() for input_ in form.find_all('input')]
        if 'password' in input_types or 'pass' in input_types:
            features["login_form"] = True
            break
        features["login_form"] = False

    # Check for external favicon
    favicon_link = soup.find('link', rel=lambda x: x and 'icon' in x.lower())
    if favicon_link:
        favicon_url = favicon_link.get('href')
        if favicon_url:
            parsed_favicon_url = urlparse(favicon_url)
            if parsed_favicon_url.netloc and parsed_favicon_url.netloc != urlparse(url).netloc:
                features["external_favicon"] = True
    # for head in soup.find_all('head'):
    #     for head.link in soup.find_all('link', href=True):


    # Count links inside HTML tags
    features["links_in_tags"] = len(soup.find_all('a'))

    # Check if form submits to an email address
    for form in forms:
        action = form.get('action')
        if (action and 'mailto:' in action) or "mailto:" in form or "mail()" in form:
            features["submit_email"] = True
            break

    # Count internal and external media files
    media_tags = soup.find_all(['img', 'audio', 'video', 'source', 'embed', 'iframe'])
    internal_media = 0
    external_media = 0
    domain = urlparse(url).netloc

    for tag in media_tags:
        media_url = tag.get('src') or tag.get('data-src')
        if media_url:
            media_url = urljoin(url, media_url)
            media_domain = urlparse(media_url).netloc
            if media_domain == domain:
                internal_media += 1
            else:
                external_media += 1

    total_media = internal_media + external_media
    if total_media > 0:
        features["ratio_intMedia"] = internal_media / total_media
        features["ratio_extMedia"] = external_media / total_media

  except requests.RequestException as e:
    print(e)
  if listOrDict:
    return features.values()
  else:
    features_overall.update(features) # Directly updates overall dictionary with current features
    return features #Returns only current features

def extract_features_per_url(index, url):
  try:
    features_overall = {}
    features_overall = initialize(features_overall)
    features_overall['index'] = index
    features_overall['url'] = url
    extract_url_components(features_overall, False)
    get_url_features(features_overall, False)
    url_char_count(features_overall, False)
    path_hostname_features(features_overall, False)
    word_features(features_overall, False)
    phish_identity_features(features_overall, False)
    whois_features(features_overall, False)
    get_web_traffic_and_index_features(features_overall, False)

    # get_redirection_and_error_features(features_overall, False)

    # Use requests to retrieve data from a given URL
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    interaction_features(features_overall, response, False)
    hyperlink_css_feaures(features_overall, soup, False)
    get_page_features(features_overall, soup, False)
    script_frame_features(features_overall, soup, False)
    # print(features_overall)
  except:
    pass
  return features_overall # Directly updates overall dictionary with current features

def final_features_extraction(input_url):
  fieldnames = [
        "index", "url", "scheme", "hostname", "path", "netloc", "domain", "subdomains", "tld",
        "length_url", "length_hostname", 'ip', 'path_extension',
        "nb_dots", "nb_hyphens", "nb_at", "nb_qm", "nb_and", "nb_or", "nb_eq", "nb_underscore", "nb_tilde", "nb_percent", "nb_slash", "nb_star", "nb_colon", "nb_comma", "nb_semicolumn", "nb_dollar", "nb_space", "nb_www", "nb_com", "nb_dslash", "nb_spl",
        "http_in_path", "https_token", "ratio_digits_url", "ratio_digits_host", "punycode", "port", "tld_in_path", "tld_in_subdomain", "abnormal_subdomain", "nb_subdomains", "prefix_suffix", "random_domain", "shortening_service",
        "length_words_raw", "char_repeat", "shortest_words_raw", "shortest_word_host", "shortest_word_path", "longest_words_raw", "longest_word_host", "longest_word_path", "avg_words_raw", "avg_word_host", "avg_word_path",
        "phish_hints", "domain_in_brand", "brand_in_subdomain", "brand_in_path", "suspicious_tld",
        "nb_hyperlinks", "ratio_intHyperlinks", "ratio_extHyperlinks", "ratio_nullHyperlinks", "ratio_safe_anchors", "nb_extCSS",
        "onmouseover", "right_click_disabled", "empty_title", "domain_in_title", "domain_with_copyright",
        "whois_registered_domain", "domain_registration_length", "domain_age",
        "web_traffic", "dns_record", "google_index",
        "sfh", "iframe", "popup_window",
        "login_form", "external_favicon", "links_in_tags", "submit_email", "ratio_intMedia", "ratio_extMedia"
      ]
  currentTime = datetime.now()
  features_overall_list = []
  for url_index in (range(len(input_url))):
    sleep(.1)
    try:
      url = input_url[url_index]
      features_overall = {}
      features_overall = extract_features_per_url(url_index, url)
      features_overall_list.append(features_overall)
      json_out = json.dumps(features_overall)
      print(json_out)
      return json_out
    except:
      pass

  # both threads completely executed
  print("Done!")
    
print("done")
